{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    pipeline\n",
    ")\n",
    "import logging\n",
    "import TextAnalysis as TA # Assumed to contain n-gram functions\n",
    "from langchain import LLMChain, HuggingFacePipeline, PromptTemplate\n",
    "from typing import Union, List\n",
    "\n",
    "\"\"\"\n",
    "Module: ADS_CasualLM.ipynb\n",
    "\n",
    "This Jupyter Notebook-style Python script demonstrates a text summarization application. It takes text files \n",
    "from a specified directory, generates summaries using a causal language model (LLM), and performs n-gram \n",
    "analysis on the generated summaries. The script uses the Hugging Face Transformers library for model loading, \n",
    "Langchain for LLM interaction and prompt management, and a custom TextAnalysis module (not shown here) for \n",
    "n-gram extraction.  The script features extensive logging for monitoring progress and handling errors.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"\n",
    "    Function: setup_logging\n",
    "\n",
    "    Configures the logging system to output log messages with timestamps to both the console and a file \n",
    "    ('model_processing.log').  This is crucial for monitoring the script's execution and identifying potential \n",
    "    problems.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: The configured logger instance.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler('model_processing.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "def setup_model(model_path: str = \"./models/Llama-3.1-8B\") -> LLMChain:\n",
    "    \"\"\"\n",
    "    Function: setup_model\n",
    "\n",
    "    Loads a causal language model (LLM) for text summarization.  This function handles loading the tokenizer, \n",
    "    the model itself, creating a text generation pipeline, and configuring a Langchain LLMChain for prompt \n",
    "    management.  It prioritizes loading the model at full precision (no quantization).  It automatically \n",
    "    selects the appropriate device (GPU if available, otherwise CPU).\n",
    "\n",
    "    Args:\n",
    "        model_path (str, optional): The path to the pre-trained LLM model directory. Defaults to \n",
    "                                    \"./models/Llama-3.1-8B\".\n",
    "\n",
    "    Returns:\n",
    "        LLMChain: A Langchain LLMChain object, ready to use for generating summaries.  This object encapsulates \n",
    "                   the model, tokenizer, and prompt template, providing a convenient interface.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Any exceptions during model loading or pipeline setup are caught, logged, and re-raised.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Initializing full precision model from path: {model_path}\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        tokenizer.pad_token = tokenizer.eos_token #Ensure pad token is set\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16  # Use float16 for memory efficiency\n",
    "        )\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "        model.to(device)\n",
    "\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            max_new_tokens=8192,  \n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature': 0.3})\n",
    "\n",
    "        template = \"\"\"\n",
    "        Write a summary of the following text delimited by triple backticks.\n",
    "        Return your response which covers the key points of the text.\n",
    "        ```{text}```\n",
    "        SUMMARY:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "        llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "        return llm_chain\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in setup_model: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "def extract_summary(llm_chain_output: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Function: extract_summary\n",
    "\n",
    "    Extracts the generated summary from the output of the LLM chain. The summary is expected to be marked \n",
    "    by \"SUMMARY:\" in the output text.\n",
    "\n",
    "    Args:\n",
    "        llm_chain_output (dict): The output dictionary from the LLMChain.run() method.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted summary text.\n",
    "    \"\"\"\n",
    "    if isinstance(llm_chain_output, dict):\n",
    "        full_output = llm_chain_output.get('text', '')\n",
    "    else:\n",
    "        full_output = llm_chain_output\n",
    "    summary_parts = full_output.split('SUMMARY:')\n",
    "    if len(summary_parts) > 1:\n",
    "        return summary_parts[1].strip()\n",
    "    return full_output.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_summaries(df: pd.DataFrame, stopwords_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function: process_summaries\n",
    "\n",
    "    Performs n-gram analysis (top words, bigrams, and trigrams) on the summaries in the input DataFrame.  \n",
    "    Uses the TextAnalysis module (not shown here) for n-gram extraction.  Handles potential errors during \n",
    "    n-gram calculation.  Provides progress updates and error logging.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing summaries.  Must have a 'Summary' column.\n",
    "        stopwords_path (str): The path to the stopwords file used for n-gram filtering.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with added columns for top words, bigrams, and trigrams.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Processing summaries for {len(df)} entries\")\n",
    "\n",
    "    try:\n",
    "        stopwords = TA.stopword_loader(stopwords_path) # Assumed function in TextAnalysis module\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            summary = row.get('Summary', '')\n",
    "            logger.debug(f\"Processing summary for index {idx}\")\n",
    "\n",
    "            if pd.isna(summary) or summary == '':\n",
    "                logger.warning(f\"Empty or NaN summary found at index {idx}\")\n",
    "                empty_result = [('', 0)] * 10  # Placeholder for empty results\n",
    "                df.loc[idx, ['Top_Words', 'Top_Bigrams', 'Top_Trigrams']] = [empty_result] * 3\n",
    "            else:\n",
    "                logger.debug(f\"Calculating n-grams for summary at index {idx}\")\n",
    "                df.loc[idx, 'Top_Words'] = TA.topwords(summary, stopwords)\n",
    "                df.loc[idx, 'Top_Bigrams'] = TA.topbigrams(summary, stopwords)\n",
    "                df.loc[idx, 'Top_Trigrams'] = TA.toptrigrams(summary, stopwords)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing summaries: {str(e)}\", exc_info=True)\n",
    "        return df # Return the DataFrame even if processing fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path: str) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Function: read_text_file\n",
    "\n",
    "    Reads the content of a text file.  Handles potential UnicodeDecodeErrors by trying both UTF-8 and \n",
    "    latin-1 encodings.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        Union[str, None]: The file's content as a string, or None if the file cannot be read.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Attempting to read file: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            logger.info(f\"Successfully read file with UTF-8 encoding: {file_path}\")\n",
    "            return content\n",
    "    except UnicodeDecodeError:\n",
    "        logger.warning(f\"UTF-8 decode failed for {file_path}, attempting with latin-1\")\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                content = file.read()\n",
    "                logger.info(f\"Successfully read file with latin-1 encoding: {file_path}\")\n",
    "                return content\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to read file with latin-1 encoding: {file_path}\", exc_info=True)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading file {file_path}: {str(e)}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_directory(llm_chain: LLMChain, directory_path: str = 'content') -> Union[pd.DataFrame, None]:\n",
    "    \"\"\"\n",
    "    Function: process_directory\n",
    "\n",
    "    Processes all '.txt' files in the specified directory, generates summaries for each using the provided \n",
    "    LLMChain, and returns the results as a Pandas DataFrame.  It handles potential errors during file \n",
    "    reading and summary generation.\n",
    "\n",
    "    Args:\n",
    "        llm_chain (LLMChain): The Langchain LLMChain object used for summary generation.\n",
    "        directory_path (str, optional): The path to the directory containing the text files. Defaults to 'content'.\n",
    "\n",
    "    Returns:\n",
    "        Union[pd.DataFrame, None]: A Pandas DataFrame containing the filenames and their generated summaries, \n",
    "                                    or None if no text files are found or an error occurs.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Processing directory: {directory_path}\")\n",
    "    \n",
    "    Path(directory_path).mkdir(exist_ok=True)\n",
    "    txt_files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]\n",
    "    \n",
    "    if not txt_files:\n",
    "        logger.warning(f\"No text files found in {directory_path}\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"Found {len(txt_files)} text files to process\")\n",
    "    summaries = []\n",
    "    \n",
    "    for file_name in txt_files:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        logger.info(f\"Processing file: {file_name}\")\n",
    "        \n",
    "        content = read_text_file(file_path)\n",
    "        if content:\n",
    "            try:\n",
    "                logger.debug(f\"Generating summary for {file_name}\")\n",
    "                summary_output = llm_chain.run(content)\n",
    "                summary = extract_summary(summary_output)\n",
    "                summaries.append({\n",
    "                    'file_name': file_name,\n",
    "                    'Summary': summary\n",
    "                })\n",
    "                logger.info(f\"Successfully processed {file_name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {file_name}: {str(e)}\", exc_info=True)\n",
    "        else:\n",
    "            logger.error(f\"Could not read content from {file_name}\")\n",
    "    \n",
    "    return pd.DataFrame(summaries) if summaries else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logger = setup_logging()\n",
    "    logger.info(\"Starting text processing application\")\n",
    "\n",
    "    try:\n",
    "        stopwords_file = \"stopwords.txt\" #Path to your stopwords file\n",
    "        llm_chain = setup_model()\n",
    "        df = process_directory(llm_chain)\n",
    "\n",
    "        if df is not None:\n",
    "            df = process_summaries(df, stopwords_file)\n",
    "            output_path = './LLM/summaries/meta_summaries.csv'\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            df.to_csv(output_path, index=False)\n",
    "            logger.info(f\"Successfully processed {len(df)} files and saved to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Fatal error in main execution\") #Log the full traceback\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "        logger.info(\"Application completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

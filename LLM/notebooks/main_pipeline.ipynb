{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ads  # Assumed to be the ADS library for astronomical data\n",
    "import operator\n",
    "import re\n",
    "import nltk\n",
    "from nltk import ngrams, word_tokenize, bigrams, trigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "import fnmatch\n",
    "import requests\n",
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "from TextAnalysis import stopword_loader, count_words, topwords, topbigrams, toptrigrams  # Custom module\n",
    "from typing import List, Union\n",
    "import logging\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "%run \"./CasualLM.ipynb\" \n",
    "\n",
    "# Directory Configuration (matching your existing structure)\n",
    "CONTENT_DIR = Path(\"./content\")\n",
    "SUMMARIES_DIR = Path(\"./summaries\")\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "MARKER_DIR = Path(\"./marker_markdown\")  # New directory for Marker markdown\n",
    "\n",
    "for directory in [CONTENT_DIR, SUMMARIES_DIR, MODELS_DIR, MARKER_DIR]:\n",
    "    directory.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Module: main_pipeline.py\n",
    "\n",
    "This module implements the main pipeline for processing a CSV file. It retrieves scientific papers based on \n",
    "arXiv identifiers from the CSV, generates summaries for each paper using either a seq2seq or causal language \n",
    "model (depending on configuration), concatenates the summaries for entries with multiple identifiers, performs \n",
    "n-gram analysis on the combined summaries, and writes the results to a new CSV file.  The summarization step \n",
    "relies on separate modules (`seq2seq.py` and `casualLM.py`).\n",
    "\"\"\"\n",
    "\n",
    "# Configure logging (good practice for larger applications)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Function: clean_text\n",
    "\n",
    "    Cleans and preprocesses the extracted text from a scientific paper.  Removes reference and acknowledgement \n",
    "    sections, text within brackets, and lines containing only single words or starting with numbers/symbols.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text extracted from a scientific paper.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and preprocessed text.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Cleaning text...\")\n",
    "    keywords = [\"REFERENCES\", \"ACKNOWLEDGEMENTS\", \"References\", \"Acknowledgements\"]\n",
    "    for keyword in keywords:\n",
    "        pos = text.find(keyword)\n",
    "        if pos != -1:\n",
    "            text = text[:pos]\n",
    "    \n",
    "    text = re.sub(r'\\([^)]*\\)|\\[[^\\]]*\\]', '', text)  # Remove bracketed content\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = [\n",
    "        line.strip() \n",
    "        for line in lines \n",
    "        if line.strip() and not re.match(r'^\\s*\\w+\\s*$', line) and not re.match(r'^[\\d\\W].*$', line)\n",
    "    ]\n",
    "    return ' '.join(cleaned_lines)\n",
    "    \n",
    "    ef get_arxiv_text(arxiv_id: str) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Function: get_arxiv_text\n",
    "\n",
    "    Downloads a PDF from arXiv given its ID, extracts the text, and cleans it using the clean_text function.\n",
    "    Handles potential errors during download and PDF processing.\n",
    "\n",
    "    Args:\n",
    "        arxiv_id (str): The arXiv ID (e.g., \"arXiv:1234.5678\").\n",
    "\n",
    "    Returns:\n",
    "        Union[str, None]: The cleaned text extracted from the PDF, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Downloading and processing arXiv ID: {arxiv_id}\")\n",
    "    arxiv_id = arxiv_id.split('arXiv:')[-1]\n",
    "    url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        pdf = PdfReader(io.BytesIO(response.content))\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return clean_text(text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error downloading PDF {url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing PDF {url}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_summary(arxiv_id: str) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Function: get_summary (Simplified)\n",
    "\n",
    "    This function is a placeholder. In a real application, this would likely call a summarization function, \n",
    "    potentially using a language model as in the previous examples.  For this example, it simply calls \n",
    "    `get_arxiv_text`.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        arxiv_id (str): The arXiv ID.\n",
    "\n",
    "    Returns:\n",
    "        Union[str, None]: The summary text, or None if an error occurs.  Currently just returns the raw text.\n",
    "    \"\"\"\n",
    "    return get_arxiv_text(arxiv_id)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_arxiv_pdf(arxiv_id: str) -> Path:\n",
    "    \"\"\"\n",
    "    Downloads PDF from arXiv and saves it to CONTENT_DIR.\n",
    "    Maintains your existing file structure.\n",
    "    \n",
    "    Args:\n",
    "        arxiv_id (str): The arXiv ID\n",
    "        \n",
    "    Returns:\n",
    "        Path: Path to the downloaded PDF file\n",
    "    \"\"\"\n",
    "    arxiv_id = arxiv_id.split('arXiv:')[-1]\n",
    "    pdf_path = CONTENT_DIR / f\"{arxiv_id}.pdf\"\n",
    "    \n",
    "    if pdf_path.exists():\n",
    "        logger.info(f\"PDF already exists: {pdf_path}\")\n",
    "        return pdf_path\n",
    "        \n",
    "    url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(pdf_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        logger.info(f\"Successfully downloaded: {pdf_path}\")\n",
    "        return pdf_path\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading PDF {url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_with_marker(pdf_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Process a PDF using Marker instead of the previous clean_text function.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (Path): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned and processed text from the PDF\n",
    "    \"\"\"\n",
    "    try:\n",
    "        converter = PdfConverter(\n",
    "            artifact_dict=create_model_dict(),\n",
    "            config={\n",
    "                \"strip_existing_ocr\": True,  # Remove existing OCR text\n",
    "                \"force_ocr\": False,  # Only OCR if needed\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        rendered = converter(str(pdf_path))\n",
    "        text, _, _ = text_from_rendered(rendered)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing PDF with Marker: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_csv(csv_filepath: str, stopwords_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function: process_csv\n",
    "\n",
    "    The main function that processes a CSV file, downloads papers, generates summaries (using a placeholder \n",
    "    function), concatenates the summaries, performs n-gram analysis, and adds the results as new columns to the \n",
    "    DataFrame.  Handles various error conditions.\n",
    "\n",
    "    Args:\n",
    "        csv_filepath (str): Path to the input CSV file.  Must contain an 'Identifier' column with a list of \n",
    "                            arXiv IDs in string format (e.g., \"['arXiv:1234.5678', 'arXiv:9876.5432']\").\n",
    "        stopwords_path (str): Path to the stopwords file for n-gram analysis.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed DataFrame with added columns for summaries and n-grams.  Returns None if \n",
    "                      errors occur during processing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filepath)\n",
    "        df['summaries'] = \"\"\n",
    "        df['topwords'] = \"\"\n",
    "        df['topbigrams'] = \"\"\n",
    "        df['toptrigrams'] = \"\"\n",
    "        llm_chain = setup_model()\n",
    "        stopwords = stopword_loader(stopwords_path) # Assumed function from TextAnalysis\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            arxiv_ids_str = row['Identifier']\n",
    "            try:\n",
    "                arxiv_ids = eval(arxiv_ids_str) # This is risky, use a safer method if possible\n",
    "                all_summaries = []\n",
    "                for arxiv_id in arxiv_ids:\n",
    "                    pdf_path = download_arxiv_pdf(arxiv_id)\n",
    "                    if pdf_path and pdf_path.exists():\n",
    "                        processed_text = process_pdf_with_marker(pdf_path)\n",
    "                        if processed_text:\n",
    "                            marker_text_path = MARKER_DIR / f\"MARKER_{arxiv_id}.md\"\n",
    "                            with open(marker_text_path, 'w', encoding='utf-8') as marker_file:\n",
    "                                marker_file.write(processed_text)\n",
    "                            # Generate summary using CausalLM\n",
    "                            summary_output = llm_chain.run(processed_text)\n",
    "                            summary = extract_summary(summary_output)\n",
    "                            all_summaries.append(summary)\n",
    "                            \n",
    "                            # Save individual summary (matching your existing pattern)\n",
    "                            summary_path = SUMMARIES_DIR / f\"SUM_{arxiv_id}.txt\"\n",
    "                            with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "                                f.write(summary)\n",
    "                        try:\n",
    "                            pdf_path.unlink()\n",
    "                            logger.info(f\"Deleted PDF: {pdf_path}\")\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Error deleting PDF {pdf_path}: {e}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Failed to download PDF for {arxiv_id}\")\n",
    "\n",
    "                if all_summaries:  # If we have any summaries\n",
    "                    df.loc[index, 'summaries'] = all_summaries\n",
    "                    \n",
    "                    combined_text = ' '.join(all_summaries)\n",
    "                    top10words = topwords(combined_text, stopwords)\n",
    "                    top10bigrams = topbigrams(combined_text, stopwords)\n",
    "                    top10trigrams = toptrigrams(combined_text, stopwords)\n",
    "                    \n",
    "                    df.loc[index, ['topwords', 'topbigrams', 'toptrigrams']] = [\n",
    "                        top10words, top10bigrams, top10trigrams\n",
    "                    ]\n",
    "                else:\n",
    "                    df.loc[index, ['summaries', 'topwords', 'topbigrams', 'toptrigrams']] = [None] * 4\n",
    "            except (SyntaxError, NameError, TypeError) as e:\n",
    "                logger.error(f\"Error processing row {index}: {e}\", exc_info=True)\n",
    "                df.loc[index, ['summaries', 'topwords', 'topbigrams', 'toptrigrams']] = [None] * 4\n",
    "\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"CSV file not found: {csv_filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "    csv_file = 'small_identifier_sample.csv'  # Replace with your CSV file\n",
    "    stopwords_file = 'stopwords.txt' # Path to your stopword file\n",
    "\n",
    "    processed_df = process_csv(csv_file, stopwords_file)\n",
    "\n",
    "    if processed_df is not None:\n",
    "        output_filename = 'combined_output.csv'\n",
    "        processed_df.to_csv(output_filename, index=False)\n",
    "        logger.info(f\"Results saved to {output_filename}\")\n",
    "    else:\n",
    "        logger.error(\"CSV processing failed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

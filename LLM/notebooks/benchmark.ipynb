{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "Module: benchmark.py (Jupyter Notebook Version)\n",
    "\n",
    "This module provides functionality for evaluating the quality of scientific text summaries using a combination \n",
    "of ROUGE and BERTScore metrics, along with a novelty score based on n-gram analysis. The primary function, \n",
    "`evaluate_scientific_summary`, calculates these metrics and returns a comprehensive evaluation score.  This \n",
    "is designed for use within a Jupyter Notebook environment.\n",
    "\"\"\"\n",
    "\n",
    "class ScientificMetricsEvaluator:\n",
    "    \"\"\"\n",
    "    Class: ScientificMetricsEvaluator\n",
    "\n",
    "    This class encapsulates the logic for evaluating scientific text summaries using multiple metrics. It \n",
    "    initializes with pre-configured scoring models and tokenizers and offers methods to preprocess text, \n",
    "    calculate ROUGE scores, BERTScores, and a novelty score based on n-grams.  The results are combined to \n",
    "    provide a holistic evaluation score.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Method: __init__\n",
    "\n",
    "        Initializes the evaluator by setting up ROUGE and BERTScore scoring mechanisms, and loading the \n",
    "        necessary tokenizer.  The BERTScore uses the 'adsabs/astroBERT' model, which is specifically \n",
    "        trained for astronomical and astrophysical literature.\n",
    "        \"\"\"\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], #ROUGE types to calculate\n",
    "            use_stemmer=True # Enables stemming to improve matching\n",
    "        )\n",
    "        \n",
    "        self.bert_scorer = BERTScorer(\n",
    "            model_type=\"adsabs/astroBERT\", #Pre-trained model for scientific text\n",
    "            num_layers=9,                  # Number of layers from the BERT model to use\n",
    "            batch_size=32,                 # Batch size for BERTScore calculation\n",
    "            nthreads=4,                    # Number of threads for parallel processing\n",
    "            all_layers=False,              # Use only the final layer for scoring\n",
    "            idf=False,                     # Don't use inverse document frequency weighting\n",
    "            lang='en',                      # Language of the text\n",
    "            rescale_with_baseline=False    # Don't rescale scores based on a baseline\n",
    "        )\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"adsabs/astroBERT\") #Tokenizer for chunking\n",
    "        self.max_length = 512             # Maximum sequence length for BERT\n",
    "        self.overlap = 50                 # Overlap between chunks (in tokens)\n",
    "\n",
    "\n",
    "    def _preprocess_text_rouge(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Method: _preprocess_text_rouge\n",
    "\n",
    "        Preprocesses text for ROUGE scoring. Removes non-alphanumeric characters, except for whitespace and \n",
    "        common punctuation marks (.,!?-). Preserves whitespace to maintain sentence structure.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed text.\n",
    "        \"\"\"\n",
    "        text = re.sub(r'[^\\w\\s,.!?-]', '', text)\n",
    "        return ' '.join(text.split()).strip()\n",
    "\n",
    "    def _preprocess_text_bert(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Method: _preprocess_text_bert\n",
    "\n",
    "        Performs minimal preprocessing for BERTScore.  Removes characters that are not alphanumeric or the \n",
    "        specified punctuation marks.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Preprocessed text.\n",
    "        \"\"\"\n",
    "        return re.sub(r'[^\\w,.!?-]', '', text)\n",
    "\n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Method: _chunk_text\n",
    "\n",
    "        Chunks a long text into smaller, overlapping sequences to accommodate the maximum sequence length \n",
    "        limit of the BERT model used in BERTScore.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to chunk.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of text chunks.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), self.max_length - self.overlap):\n",
    "            chunk_tokens = tokens[i:i + self.max_length]\n",
    "            chunk_tokens = [self.tokenizer.cls_token_id] + chunk_tokens + [self.tokenizer.sep_token_id]\n",
    "            chunk = self.tokenizer.decode(chunk_tokens)\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def calculate_rouge(self, reference: str, candidate: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Method: calculate_rouge\n",
    "\n",
    "        Calculates ROUGE scores (rouge1, rouge2, rougeL, rougeLsum) between a reference and a candidate text.\n",
    "\n",
    "        Args:\n",
    "            reference (str): The reference text.\n",
    "            candidate (str): The candidate text (e.g., a generated summary).\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: A dictionary of ROUGE scores.\n",
    "        \"\"\"\n",
    "        reference = self._preprocess_text_rouge(reference)\n",
    "        candidate = self._preprocess_text_rouge(candidate)\n",
    "        scores = self.rouge_scorer.score(reference, candidate)\n",
    "        return {k: v.fmeasure for k, v in scores.items()}\n",
    "\n",
    "    def calculate_bertscore(self, reference: str, candidate: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Method: calculate_bertscore\n",
    "\n",
    "        Calculates BERTScore (precision, recall, F1) using the chunked text.  This method accounts for \n",
    "        the maximum sequence length constraints of the BERT model by dividing the text into overlapping \n",
    "        chunks before computing the BERTScore.  The scores are then averaged across all chunk pairs.\n",
    "\n",
    "        Args:\n",
    "            reference (str): The reference text.\n",
    "            candidate (str): The candidate text.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: A dictionary of BERTScore (precision, recall, f1).\n",
    "        \"\"\"\n",
    "        reference = self._preprocess_text_bert(reference)\n",
    "        candidate = self._preprocess_text_bert(candidate)\n",
    "        reference_chunks = self._chunk_text(reference)\n",
    "        candidate_chunks = self._chunk_text(candidate)\n",
    "        chunk_scores = []\n",
    "        for ref_chunk in reference_chunks:\n",
    "            for cand_chunk in candidate_chunks:\n",
    "                P, R, F1 = self.bert_scorer.score([cand_chunk], [ref_chunk])\n",
    "                chunk_scores.append((float(P[0]), float(R[0]), float(F1[0])))\n",
    "        if chunk_scores:\n",
    "            avg_precision = np.mean([score[0] for score in chunk_scores])\n",
    "            avg_recall = np.mean([score[1] for score in chunk_scores])\n",
    "            avg_f1 = np.mean([score[2] for score in chunk_scores])\n",
    "        else:\n",
    "            avg_precision = avg_recall = avg_f1 = 0.0\n",
    "        return {'precision': avg_precision, 'recall': avg_recall, 'f1': avg_f1}\n",
    "\n",
    "    def calculate_ngram_novelty(self, reference: str, candidate: str) -> float:\n",
    "        \"\"\"\n",
    "        Method: calculate_ngram_novelty\n",
    "\n",
    "        Calculates a novelty score by determining the proportion of n-grams (unigrams, bigrams, trigrams) \n",
    "        in the candidate text that are not present in the reference text.  A higher novelty score indicates \n",
    "        that the candidate text contains more unique information relative to the reference.\n",
    "\n",
    "        Args:\n",
    "            reference (str): The reference text.\n",
    "            candidate (str): The candidate text.\n",
    "\n",
    "        Returns:\n",
    "            float: The novelty score (average across unigrams, bigrams, and trigrams).\n",
    "        \"\"\"\n",
    "        reference = self._preprocess_text_rouge(reference)\n",
    "        candidate = self._preprocess_text_rouge(candidate)\n",
    "        def get_ngrams(text, n):\n",
    "            words = text.split()\n",
    "            return set(' '.join(words[i:i+n]) for i in range(len(words)-n+1))\n",
    "        novelty_scores = []\n",
    "        for n in [1, 2, 3]:\n",
    "            ref_ngrams = get_ngrams(reference, n)\n",
    "            cand_ngrams = get_ngrams(candidate, n)\n",
    "            novel_ngrams = cand_ngrams - ref_ngrams\n",
    "            if cand_ngrams:\n",
    "                novelty_scores.append(len(novel_ngrams) / len(cand_ngrams))\n",
    "        return np.mean(novelty_scores) if novelty_scores else 0.0\n",
    "\n",
    "    def evaluate_summary(self, reference: str, candidate: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Method: evaluate_summary\n",
    "\n",
    "        Computes all the evaluation metrics (ROUGE, BERTScore, and novelty) and combines them into a single \n",
    "        'abstractive_score'.  The weights are specifically chosen to favor summaries that are both semantically \n",
    "        similar to the reference and also offer new insights (high novelty).\n",
    "\n",
    "        Args:\n",
    "            reference (str): The reference text.\n",
    "            candidate (str): The candidate summary text.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: A dictionary containing all individual metric scores and the combined \n",
    "                              'abstractive_score'.\n",
    "        \"\"\"\n",
    "        rouge_scores = self.calculate_rouge(reference, candidate)\n",
    "        bert_scores = self.calculate_bertscore(reference, candidate)\n",
    "        novelty_score = self.calculate_ngram_novelty(reference, candidate)\n",
    "        \n",
    "        final_scores = {\n",
    "            'rouge1': rouge_scores['rouge1'],\n",
    "            'rouge2': rouge_scores['rouge2'],\n",
    "            'rougeL': rouge_scores['rougeL'],\n",
    "            'rougeLsum': rouge_scores['rougeLsum'],\n",
    "            'bertscore_precision': bert_scores['precision'],\n",
    "            'bertscore_recall': bert_scores['recall'],\n",
    "            'bertscore_f1': bert_scores['f1'],\n",
    "            'ngram_novelty': novelty_score\n",
    "        }\n",
    "        \n",
    "        # Weights optimized for abstractive summaries (high semantic similarity and high novelty)\n",
    "        weights = {\n",
    "            'bertscore_f1': 0.5,      # Semantic similarity\n",
    "            'ngram_novelty': 0.5       # Novelty of information\n",
    "        }\n",
    "        \n",
    "        final_scores['abstractive_score'] = sum(\n",
    "            final_scores[metric] * weight for metric, weight in weights.items()\n",
    "        )\n",
    "        return final_scores\n",
    "\n",
    "def evaluate_scientific_summary(reference_text: str, summary_text: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Function: evaluate_scientific_summary\n",
    "\n",
    "    A convenience function that wraps the `ScientificMetricsEvaluator` to make it easier to evaluate summaries.\n",
    "\n",
    "    Args:\n",
    "        reference_text (str): The original text.\n",
    "        summary_text (str): The generated summary.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    evaluator = ScientificMetricsEvaluator()\n",
    "    return evaluator.evaluate_summary(reference_text, summary_text)\n",
    "\n",
    "# Example usage (within a Jupyter Notebook cell):\n",
    "# scores = evaluate_scientific_summary(paper, summary)\n",
    "# print(scores)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
